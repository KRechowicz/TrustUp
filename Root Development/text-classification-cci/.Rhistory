# this runs pretty quickly
tos_rf = train(Vendor ~ ., data = train_data,
method = "rf", ntree = 200,
trControl = trainControl(method = "oob"))
# get the predictions to play with classification threshold
results = data.frame(actual = test_data$Vendor, predict(tos_rf, test_data, type = "prob"))
View(results)
confusionMatrix(predict(tos_rf, test_data), test_data$Vendor)
confusionMatrix(predict(tos_rf, test_data), test_data$Vendor)
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
tos_model = readRDS("models/terms-of-service/tos_rf.RDS")
whoop_filename = "input/terms-of-service/whoop-tos.txt"
fitbit_filename = "input/terms-of-service/fitbit-tos.txt"
cellartracker_filename = "input/terms-of-service/cellartracker-tos.txt"
filename_of_tos_txt_document_to_classify = fitbit_filename
tos_document_to_classify_as_tbl = read_txt_file_into_tibble(filename_of_tos_txt_document_to_classify)
View(tos_document_to_classify_as_tbl)
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
View(tos)data)
View(tos_data)
# unnest all the tokens
tos_tokens = tos_document_to_classify_as_tbl %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# model variables
model_vars = colnames(tos_model$trainingData)
model_vars = model_vars[2:length(model_vars)]
# model variables
model_vars = colnames(tos_model$trainingData)
model_vars = model_vars[2:length(model_vars)]
# tweets with entries we can classify are
rows_we_can_classify = tos_tokens$ID %>% unique()
tos_document_to_classify_as_tbl_for_classification = tos_document_to_classify_as_tbl[rows_we_can_classify,]
# create a document-term matrix with all features and tf weighting
# you can ignore the warning
tos_dtm = count(tos_tokens, ID, word)
tos_dtm = cast_dtm(tos_dtm, document = ID, term = word, value = n)
# remove any tokens that are missing from more than 99% of the documents in the corpus
tos_dtm = removeSparseTerms(tos_dtm, sparse = .99)
model_data = cbind.data.frame(as.matrix(tos_dtm))
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
tos_model = readRDS("models/terms-of-service/tos_rf.RDS")
whoop_filename = "input/terms-of-service/whoop-tos.txt"
fitbit_filename = "input/terms-of-service/fitbit-tos.txt"
cellartracker_filename = "input/terms-of-service/cellartracker-tos.txt"
filename_of_tos_txt_document_to_classify = fitbit_filename
tos_document_to_classify_as_tbl = read_txt_file_into_tibble(filename_of_tos_txt_document_to_classify)
# unnest all the tokens
tos_tokens = tos_document_to_classify_as_tbl %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# model variables
model_vars = colnames(tos_model$trainingData)
model_vars = model_vars[2:length(model_vars)]
# tweets with entries we can classify are
rows_we_can_classify = tos_tokens$ID %>% unique()
tos_document_to_classify_as_tbl_for_classification = tos_document_to_classify_as_tbl[rows_we_can_classify,]
# create a document-term matrix with all features and tf weighting
# you can ignore the warning
tos_dtm = count(tos_tokens, ID, word)
tos_dtm = cast_dtm(tos_dtm, document = ID, term = word, value = n)
# remove any tokens that are missing from more than 99% of the documents in the corpus
tos_dtm = removeSparseTerms(tos_dtm, sparse = .99)
model_data = cbind.data.frame(as.matrix(tos_dtm))
needed_names = setdiff(model_vars, tos_tokens$word)
needed_names = c(needed_names, setdiff(model_vars, colnames(model_data)))
needed_names = needed_names %>% unique()
for (i in 1:length(needed_names))
{
colnames_of_interest = needed_names[i]
model_data[,colnames_of_interest] = 0
}
model_data = model_data[,model_vars]
classifications_for_tos_document = predict(tos_model, model_data)
classifications_for_tos_document
tos_document_to_classify_as_tbl_for_classification$Vendor = classifications_for_tos_document
View(tos_document_to_classify_as_tbl_for_classification)
tos_document_summary = tos_document_to_classify_as_tbl_for_classification %>% group_by(Vendor) %>% summarise(VendorProbability = n()) %>% arrange(desc(VendorProbability))
tos_document_summary = tos_document_summary %>% mutate(VendorProbability = VendorProbability / sum(VendorProbability))
View(tos_document_summary)
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(ID, word)
tos_dtm
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(ID, word)
tos_dtm = tos_dtm %>% cast_dtm(document = ID, term = word, value = n)
# create a document-term matrix with all features and tf-idf weighting
tos_dtm_tf_idf <- tos_tokens %>% bind_tf_idf(word, chunk, n)
tos_dtm_tf_idf <- tos_dtm_tf_idf %>% cast_dtm(document = ID, term = word, value = tf_idf)
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(ID, word)
tos_dtm = tos_dtm %>% cast_dtm(document = ID, term = word, value = n)
# create a document-term matrix with all features and tf-idf weighting
tos_dtm_tf_idf <- tos_tokens %>% bind_tf_idf(word, chunk, n)
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(ID, word)
tos_dtm = tos_dtm %>% cast_dtm(document = ID, term = word, value = n)
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(ID, word)
tos_dtm
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(ID, word)
tos_dtm = tos_dtm %>% cast_dtm(document = ID, term = word, value = n)
# create a document-term matrix with all features and tf-idf weighting
tos_dtm_tf_idf = tos_tokens %>% bind_tf_idf(word, ID, n)
?bind_tf_idf
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(ID, word)
tos_dtm = tos_dtm %>% cast_dtm(document = ID, term = word, value = n)
tos_dtm
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(ID, word)
tos_dtm = tos_dtm %>% cast_dtm(document = ID, term = word, value = n)
# create a document-term matrix with all features and tf-idf weighting
tos_dtm_tf_idf = tos_tokens %>% count(ID, word) %>% bind_tf_idf(word, ID, n)
tos_dtm_tf_idf
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
tos_data
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
tos_tokens
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(Vendor, word)
tos_dtm = tos_dtm %>% cast_dtm(document = Vendor, term = word, value = n)
# create a document-term matrix with all features and tf-idf weighting
tos_dtm_tf_idf = tos_tokens %>% count(Vendor, word) %>% bind_tf_idf(word, ID, n)
tos_dtm_tf_idf = tos_dtm_tf_idf %>% cast_dtm(document = Vendor, term = word, value = tf_idf)
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(Vendor, word)
tos_dtm = tos_dtm %>% cast_dtm(document = Vendor, term = word, value = n)
# create a document-term matrix with all features and tf-idf weighting
tos_dtm_tf_idf = tos_tokens %>% count(Vendor, word) %>% bind_tf_idf(word, Vendor, n)
tos_dtm_tf_idf = tos_dtm_tf_idf %>% cast_dtm(document = Vendor, term = word, value = tf_idf)
tos_dtm_tf_idf
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(Vendor, word)
tos_dtm = tos_dtm %>% cast_dtm(document = Vendor, term = word, value = n)
# create a document-term matrix with all features and tf-idf weighting
tos_dtm_tf_idf = tos_tokens %>% count(Vendor, word) %>% bind_tf_idf(word, Vendor, n)
tos_dtm_tf_idf = tos_dtm_tf_idf %>% cast_dtm(document = Vendor, term = word, value = tf_idf)
# remove any tokens that are missing from more than 99% of the documents in the corpus
tos_dtm = tos_dtm %>% removeSparseTerms(sparse = .99)
# at this point we have filtered out some of the original text that have not met our guidelines
# (only contains strings with numbers or of length less than 5)
# so we need to figure out which text rows are left, we can do this with group_by
# followed by a meaningless summarise. The goal here is to get the vendor
# labels for the messages left in our data set
vendors_i_want = tos_tokens %>% group_by(ID, Vendor) %>% summarise(n=n())
# build the model data
model_data = cbind.data.frame(as.matrix(tos_dtm), as.factor(vendors_i_want$Vendor))
number_of_cols = ncol(model_data)
colnames(model_data) = c(colnames(model_data)[1:(number_of_cols-1)], "Vendor")
# split train and test
index = createDataPartition(model_data$Vendor, p = 0.7, list = FALSE)
train_data = model_data[index, ]
test_data  = model_data[-index, ]
# this runs pretty quickly
tos_rf = train(Vendor ~ ., data = train_data,
method = "rf", ntree = 200,
trControl = trainControl(method = "oob"))
# get the predictions to play with classification threshold
results = data.frame(actual = test_data$Vendor, predict(tos_rf, test_data, type = "prob"))
confusionMatrix(predict(tos_rf, test_data), test_data$Vendor)
saveRDS(tos_rf, file="models/terms-of-service/tos_rf.RDS")
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(Vendor, word)
tos_dtm = tos_dtm %>% cast_dtm(document = Vendor, term = word, value = n)
# create a document-term matrix with all features and tf-idf weighting
tos_dtm_tf_idf = tos_tokens %>% count(Vendor, word) %>% bind_tf_idf(word, Vendor, n)
tos_dtm_tf_idf = tos_dtm_tf_idf %>% cast_dtm(document = Vendor, term = word, value = tf_idf)
# remove any tokens that are missing from more than 99% of the documents in the corpus
tos_dtm = tos_dtm %>% removeSparseTerms(sparse = .99)
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(Vendor, word)
tos_dtm = tos_dtm %>% cast_dtm(document = Vendor, term = word, value = n)
# create a document-term matrix with all features and tf-idf weighting
tos_dtm_tf_idf = tos_tokens %>% count(Vendor, word) %>% bind_tf_idf(word, Vendor, n)
tos_dtm_tf_idf = tos_dtm_tf_idf %>% cast_dtm(document = Vendor, term = word, value = tf_idf)
# remove any tokens that are missing from more than 99% of the documents in the corpus
tos_dtm = tos_dtm %>% removeSparseTerms(sparse = .99)
# at this point we have filtered out some of the original text that have not met our guidelines
# (only contains strings with numbers or of length less than 5)
# so we need to figure out which text rows are left, we can do this with group_by
# followed by a meaningless summarise. The goal here is to get the vendor
# labels for the messages left in our data set
vendors_i_want = tos_tokens %>% group_by(ID, Vendor) %>% summarise(n=n())
model_data = cbind.data.frame(as.matrix(tos_dtm), as.factor(vendors_i_want$Vendor))
number_of_cols = ncol(model_data)
colnames(model_data) = c(colnames(model_data)[1:(number_of_cols-1)], "Vendor")
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(ID, word)
tos_dtm = tos_dtm %>% cast_dtm(document = ID, term = word, value = n)
# create a document-term matrix with all features and tf-idf weighting
tos_dtm_tf_idf = tos_tokens %>% count(ID, word) %>% bind_tf_idf(word, ID, n)
tos_dtm_tf_idf = tos_dtm_tf_idf %>% cast_dtm(document = ID, term = word, value = tf_idf)
# remove any tokens that are missing from more than 99% of the documents in the corpus
tos_dtm = tos_dtm %>% removeSparseTerms(sparse = .99)
# at this point we have filtered out some of the original text that have not met our guidelines
# (only contains strings with numbers or of length less than 5)
# so we need to figure out which text rows are left, we can do this with group_by
# followed by a meaningless summarise. The goal here is to get the vendor
# labels for the messages left in our data set
vendors_i_want = tos_tokens %>% group_by(ID, Vendor) %>% summarise(n=n())
# build the model data
model_data = cbind.data.frame(as.matrix(tos_dtm), as.factor(vendors_i_want$Vendor))
number_of_cols = ncol(model_data)
colnames(model_data) = c(colnames(model_data)[1:(number_of_cols-1)], "Vendor")
# split train and test
index = createDataPartition(model_data$Vendor, p = 0.7, list = FALSE)
train_data = model_data[index, ]
test_data  = model_data[-index, ]
# this runs pretty quickly
tos_rf = train(Vendor ~ ., data = train_data,
method = "rf", ntree = 200,
trControl = trainControl(method = "oob"))
# get the predictions to play with classification threshold
results = data.frame(actual = test_data$Vendor, predict(tos_rf, test_data, type = "prob"))
confusionMatrix(predict(tos_rf, test_data), test_data$Vendor)
saveRDS(tos_rf, file="models/terms-of-service/tos_rf.RDS")
library(tidyverse)
library(tm)
library(caret)
library(tidytext)
source("code/text-classification-api.R")
# set the random number seed for development
set.seed(42)
# get all the tos data in a single table
tos_data = read_all_files_in_dir_into_tibble("data/terms-of-service/", replace_vendor=TRUE)
# unnest all the tokens
tos_tokens = tos_data %>% unnest_tokens(output = word, input = Text)
# filter out any text that contains a digit anywhere or is not of length 5
tos_tokens = tos_tokens %>% filter(str_detect(word, "^\\D+$") & str_detect(word, "\\w{5,}"))
tos_tokens = tos_tokens %>% anti_join(stop_words, by="word")
tos_tokens = tos_tokens %>% mutate(word = SnowballC::wordStem(word))
# create a document-term matrix with all features and tf weighting
tos_dtm = tos_tokens %>% count(ID, word)
tos_dtm = tos_dtm %>% cast_dtm(document = ID, term = word, value = n)
# create a document-term matrix with all features and tf-idf weighting
tos_dtm_tf_idf = tos_tokens %>% count(ID, word) %>% bind_tf_idf(word, ID, n)
tos_dtm_tf_idf = tos_dtm_tf_idf %>% cast_dtm(document = ID, term = word, value = tf_idf)
# remove any tokens that are missing from more than 99% of the documents in the corpus
tos_dtm = tos_dtm %>% removeSparseTerms(sparse = .99)
tos_dtm_tf_idf = tos_dtm_tf_idf %>% removeSparseTerms(sparse = .99)
# at this point we have filtered out some of the original text that have not met our guidelines
# (only contains strings with numbers or of length less than 5)
# so we need to figure out which text rows are left, we can do this with group_by
# followed by a meaningless summarise. The goal here is to get the vendor
# labels for the messages left in our data set
vendors_i_want = tos_tokens %>% group_by(ID, Vendor) %>% summarise(n=n())
# build the model data
model_data = cbind.data.frame(as.matrix(tos_dtm_tf_idf), as.factor(vendors_i_want$Vendor))
number_of_cols = ncol(model_data)
colnames(model_data) = c(colnames(model_data)[1:(number_of_cols-1)], "Vendor")
# split train and test
index = createDataPartition(model_data$Vendor, p = 0.7, list = FALSE)
train_data = model_data[index, ]
test_data  = model_data[-index, ]
# this runs pretty quickly
tos_rf = train(Vendor ~ ., data = train_data,
method = "rf", ntree = 200,
trControl = trainControl(method = "oob"))
# get the predictions to play with classification threshold
results = data.frame(actual = test_data$Vendor, predict(tos_rf, test_data, type = "prob"))
confusionMatrix(predict(tos_rf, test_data), test_data$Vendor)
saveRDS(tos_rf, file="models/terms-of-service/tos_rf.RDS")
